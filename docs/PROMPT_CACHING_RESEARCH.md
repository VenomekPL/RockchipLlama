# Prompt Caching Research - Technocore RKLLM Implementation

**Source**: `/home/angeiv/AI/technocore/modules/rkllm/`  
**Date**: October 20, 2025  
**Purpose**: Analysis of existing prompt caching implementation for RockchipLlama Phase 4.1

---

## ğŸ¯ Executive Summary

The Technocore RKLLM server implements a **model-specific prompt caching system** that:
- âœ… Organizes caches by model name (`/cache/{model_name}/`)
- âœ… Saves both text and binary formats
- âœ… Includes metadata (creation time, content length)
- âœ… Provides cache list/load/save API
- âœ… Auto-detects model context sizes from filenames
- âœ… Uses conservative memory limits for RK3588

---

## ğŸ“ Architecture Overview

### Cache Directory Structure
```
/cache/
â”œâ”€â”€ qwen3-0.6b/
â”‚   â”œâ”€â”€ system_default.txt         # Human-readable cache
â”‚   â”œâ”€â”€ system_default.bin         # RKLLM binary cache
â”‚   â”œâ”€â”€ system_default.json        # Metadata
â”‚   â”œâ”€â”€ coding_assistant.txt
â”‚   â”œâ”€â”€ coding_assistant.bin
â”‚   â””â”€â”€ coding_assistant.json
â”œâ”€â”€ qwen3-4b/
â”‚   â””â”€â”€ ...
â””â”€â”€ gemma3-1b/
    â””â”€â”€ ...
```

### Metadata Format
```json
{
  "cache_name": "system_default",
  "model_name": "qwen3-0.6b",
  "created_at": 1729450123.456,
  "content_length": 1250
}
```

---

## ğŸ”§ Key Implementation Details

### 1. Model-Specific Cache Resolution

```python
def _resolve_cache_path_for_current_model(self) -> str:
    """Get the cache directory for the currently loaded model"""
    if not self.loaded_model_name:
        raise RuntimeError("No model loaded")
    return os.path.join(self.cache_base_dir, self.loaded_model_name)
```

**Key Insight**: Each model gets its own cache directory, preventing cross-model cache pollution.

---

### 2. Cache Save Operation

```python
def save_prompt_cache(self, cache_name: str, prompt_content: str) -> None:
    """Save a prompt cache with name-based organization"""
    if not self.loaded_model_name:
        raise RuntimeError("No model loaded")
        
    model_cache_dir = self._resolve_cache_path_for_current_model()
    os.makedirs(model_cache_dir, exist_ok=True)
    
    # Save as both text and binary (for RKLLM compatibility)
    cache_path_text = os.path.join(model_cache_dir, f"{cache_name}.txt")
    cache_path_bin = os.path.join(model_cache_dir, f"{cache_name}.bin") 
    
    # Save text version for readability
    with open(cache_path_text, 'w', encoding='utf-8') as f:
        f.write(prompt_content)
        
    # Save metadata
    metadata = {
        "cache_name": cache_name,
        "model_name": self.loaded_model_name,
        "created_at": time.time(),
        "content_length": len(prompt_content)
    }
    metadata_path = os.path.join(model_cache_dir, f"{cache_name}.json")
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2)
        
    # Create binary placeholder for RKLLM (actual binary caching may require runtime support)
    with open(cache_path_bin, 'wb') as f:
        f.write(prompt_content.encode('utf-8'))
        
    print(f"[INFO] Saved prompt cache '{cache_name}' for model '{self.loaded_model_name}'")
```

**Key Features**:
- âœ… Dual format: text (debugging) + binary (RKLLM runtime)
- âœ… Metadata tracking for cache management
- âœ… Automatic directory creation
- âœ… Model-scoped naming

---

### 3. Cache Load Operation

```python
def load_prompt_cache(self, cache_name: str) -> Optional[str]:
    """Load a prompt cache by name"""
    if not self.loaded_model_name:
        raise RuntimeError("No model loaded")
        
    model_cache_dir = self._resolve_cache_path_for_current_model()
    cache_path_text = os.path.join(model_cache_dir, f"{cache_name}.txt")
    
    if os.path.exists(cache_path_text):
        try:
            with open(cache_path_text, 'r', encoding='utf-8') as f:
                content = f.read()
            print(f"[INFO] Loaded cached prompt '{cache_name}' from {self.loaded_model_name}")
            return content
        except Exception as e:
            print(f"[WARN] Failed to load cached prompt '{cache_name}': {e}")
            return None
    else:
        print(f"[WARN] Cached prompt '{cache_name}' not found for model '{self.loaded_model_name}'")
        return None
```

**Key Features**:
- âœ… Simple string return (easy to use)
- âœ… Graceful error handling
- âœ… Automatic model scope checking

---

### 4. Cache Listing

```python
def list_cached_prompts(self) -> List[Dict[str, Any]]:
    """List all cached prompts for the current model"""
    if not self.loaded_model_name:
        return []
        
    model_cache_dir = self._resolve_cache_path_for_current_model()
    if not os.path.exists(model_cache_dir):
        return []
        
    cached_prompts = []
    try:
        for metadata_file in glob.glob(os.path.join(model_cache_dir, "*.json")):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                cached_prompts.append(metadata)
            except Exception as e:
                print(f"[WARN] Failed to read metadata from {metadata_file}: {e}")
    except Exception as e:
        print(f"[WARN] Failed to list cached prompts: {e}")
        
    return sorted(cached_prompts, key=lambda x: x.get('created_at', 0), reverse=True)

def list_all_cached_prompts(self) -> Dict[str, List[Dict[str, Any]]]:
    """List cached prompts for all models"""
    all_caches = {}
    
    try:
        if os.path.exists(self.cache_base_dir):
            for model_dir in os.listdir(self.cache_base_dir):
                model_cache_dir = os.path.join(self.cache_base_dir, model_dir)
                if os.path.isdir(model_cache_dir):
                    model_caches = []
                    for metadata_file in glob.glob(os.path.join(model_cache_dir, "*.json")):
                        try:
                            with open(metadata_file, 'r', encoding='utf-8') as f:
                                metadata = json.load(f)
                            model_caches.append(metadata)
                        except Exception:
                            pass
                    if model_caches:
                        all_caches[model_dir] = sorted(model_caches, key=lambda x: x.get('created_at', 0), reverse=True)
    except Exception as e:
        print(f"[WARN] Failed to list all cached prompts: {e}")
        
    return all_caches
```

**Key Features**:
- âœ… Per-model listing
- âœ… Cross-model listing
- âœ… Sorted by creation time (newest first)
- âœ… Robust error handling

---

### 5. Cache Clearing

```python
def clear_cache(self) -> None:
    """Clear KV cache and prompt cache"""
    if not self.handle:
        raise RuntimeError("Model not loaded")
        
    # Method 1: Clear KV cache if available
    if self._rkllm_clear_kv_cache is not None:
        try:
            # Clear all cache (pass -1 for all sequences)
            ret = self._rkllm_clear_kv_cache(self.handle, -1, None, None)
            if ret == 0:
                print(f"[INFO] Cleared KV cache for model {self.loaded_model_name}")
            else:
                print(f"[WARN] Failed to clear KV cache, code: {ret}")
        except Exception as e:
            print(f"[WARN] Error clearing KV cache: {e}")
    
    # Method 2: Reset chat template to clear system prompt cache
    try:
        self.set_chat_template("", "", "")
        self._system_prompt = None
        self._prompt_prefix = None  
        self._prompt_postfix = None
        print(f"[INFO] Cleared chat template for model {self.loaded_model_name}")
    except Exception as e:
        print(f"[WARN] Error clearing chat template: {e}")
```

**Key Features**:
- âœ… Two-method approach (RKLLM native + template reset)
- âœ… Graceful fallback if RKLLM API unavailable
- âœ… Clear system prompt state

---

## ğŸ¨ RKLLM Prompt Cache Structures

### RKLLMPromptCacheParam (C Structure)

```python
class RKLLMPromptCacheParam(ctypes.Structure):
    _fields_ = [
        ("save_prompt_cache", ctypes.c_int),      # 1 = save cache
        ("prompt_cache_path", ctypes.c_char_p),   # Path to cache file
    ]
```

### RKLLMInferParam (with cache support)

```python
class RKLLMInferParam(ctypes.Structure):
    _fields_ = [
        ("mode", ctypes.c_int),                                      # Inference mode
        ("lora_params", ctypes.POINTER(RKLLMLoraParam)),            # LoRA adapters
        ("prompt_cache_params", ctypes.POINTER(RKLLMPromptCacheParam)),  # ğŸ”¥ Cache params
        ("keep_history", ctypes.c_int),                             # Keep conversation history
    ]
```

**Usage Pattern**:
```python
infer_param = RKLLMInferParam()
infer_param.mode = RKLLM_INFER_GENERATE
infer_param.prompt_cache_params = None  # No caching by default
infer_param.keep_history = 0  # Stateless (client manages context)
```

---

## ğŸš€ Context Size Detection

### Auto-Detection Logic

```python
def _detect_model_context_size(self, model_path: str) -> int:
    """
    Auto-detect context size based on model capabilities AND hardware constraints

    âš ï¸  CRITICAL: RK3588 has limited memory - be conservative with context sizes
    - Qwen3-4B: Use 4K context to avoid memory overflow
    - Qwen3-0.6B: Can use up to 16K
    - RKLLM Runtime has a HARD LIMIT of 16K context
    """
    filename = os.path.basename(model_path).lower()

    # RKLLM Runtime has a HARD LIMIT of 16K context
    RKLLM_MAX_CONTEXT_LIMIT = 16384

    # Qwen3 models - constrained by RK3588 memory limitations
    if 'qwen3' in filename:
        if '0.6b' in filename:
            # Qwen3-0.6B can handle 16K on RK3588
            return min(16384, RKLLM_MAX_CONTEXT_LIMIT)
        elif '4b' in filename:
            # Qwen3-4B: Use conservative 4K context to avoid memory overflow on RK3588
            return 4096  # Conservative for 4B model on RK3588
        elif '8b' in filename:
            # Qwen3-8B: Use conservative 8K context
            return 8192  # Conservative for larger models
        elif '30b' in filename or '235b' in filename:
            # Qwen3 large models: Use minimal 4K context
            return 4096  # Very conservative for large models
        else:
            return 4096  # Safe default

    # Other model families - also constrained by RK3588 limits
    elif 'qwen2.5' in filename:
        if '0.5b' in filename or '1.5b' in filename:
            return min(8192, RKLLM_MAX_CONTEXT_LIMIT)  # Conservative
        elif '3b' in filename or '7b' in filename:
            return min(4096, RKLLM_MAX_CONTEXT_LIMIT)  # Very conservative
        else:
            return 4096  # Safe default
    else:
        return 4096  # Safe default for unknown models

    # Check for explicit context size in filename (fallback)
    import re
    ctx_match = re.search(r'ctx(\d+)', filename)
    if ctx_match:
        requested_context = int(ctx_match.group(1))
        # Never exceed RKLLM's hard limit
        return min(requested_context, RKLLM_MAX_CONTEXT_LIMIT)

    # Default fallback - use RKLLM's safe limit
    return RKLLM_MAX_CONTEXT_LIMIT
```

**Key Insights**:
- âš ï¸ **RK3588 Memory Constraint**: Large models need smaller contexts
- âœ… **RKLLM Hard Limit**: 16K max regardless of model
- âœ… **Qwen3-4B**: Forced to 4K to prevent OOM
- âœ… **Qwen3-0.6B**: Can use full 16K

---

## ğŸ“Š Friendly Model Names

### Name Extraction Logic

```python
def _extract_model_name(self, model_path: str) -> str:
    """Extract a model name for cache directory organization"""
    filename = os.path.basename(model_path)
    # Remove .rkllm extension and use as directory name
    model_name = filename.replace('.rkllm', '')
    return model_name

def _create_friendly_name(self, filename: str) -> str:
    """Create a friendly name from RKLLM filename"""
    # Remove .rkllm extension
    name = filename.replace('.rkllm', '').lower()
    
    # Extract key components for common patterns
    if 'qwen3' in name and '0.6b' in name:
        base = 'qwen3-0.6b'
    elif 'qwen3' in name and '4b' in name:
        base = 'qwen3-4b'
    elif 'qwen2' in name and '0.5b' in name:
        base = 'qwen2-0.5b'
    elif 'qwen2' in name and '1.5b' in name:
        base = 'qwen2-1.5b'
    else:
        # Fallback: use first part before version/config
        parts = name.replace('_', '-').split('-')
        base = parts[0] if parts else name
        
    # Add optimization level
    if 'opt0' in name:
        base += '-opt0'
    elif 'opt1' in name:
        base += '-opt1'
        
    # Add hybrid mode
    if 'hybrid0' in name:
        base += '-hybrid0'
    elif 'hybrid1' in name:
        base += '-hybrid1'
        
    return base
```

**Examples**:
- `Qwen_Qwen3-0.6B-w8a8-opt0-hybrid0-npu3-ctx16384-rk3588.rkllm` â†’ `qwen3-0.6b-opt0-hybrid0`
- `Qwen3-4B-Instruct-2507-rk3588-w8a8-opt-0-hybrid-ratio-0.0-16k.rkllm` â†’ `qwen3-4b`

---

## ğŸ”¥ Chat Template Support

### System Prompt Management

```python
def set_chat_template(self, system_prompt: str, prompt_prefix: str = "", prompt_postfix: str = "") -> None:
    """Set chat template on the loaded model"""
    if not self.handle:
        raise RuntimeError("Model not loaded")
    
    ret = self.rkllm_set_chat_template(
        self.handle,
        ctypes.c_char_p(system_prompt.encode("utf-8")),
        ctypes.c_char_p(prompt_prefix.encode("utf-8")),
        ctypes.c_char_p(prompt_postfix.encode("utf-8")),
    )
    if ret != 0:
        raise RuntimeError(f"rkllm_set_chat_template failed with code {ret}")
    
    self._system_prompt = system_prompt
    self._prompt_prefix = prompt_prefix
    self._prompt_postfix = prompt_postfix
```

**Key Features**:
- âœ… Native RKLLM chat template API
- âœ… System prompt persistence
- âœ… Prefix/postfix for formatting
- âœ… Re-applied on model load

---

## ğŸ’¡ Implementation Recommendations for RockchipLlama

### Phase 4.1 Implementation Plan

#### 1. **Adopt Model-Specific Cache Structure** âœ…
```
cache/
â”œâ”€â”€ qwen3-0.6b/
â”‚   â”œâ”€â”€ system_default.txt
â”‚   â”œâ”€â”€ system_default.json
â”‚   â””â”€â”€ system_coding.txt
â”œâ”€â”€ gemma3-1b/
â”‚   â””â”€â”€ ...
```

#### 2. **Implement Core Cache Methods** âœ…
- `save_prompt_cache(cache_name, content)` - Save to model-specific directory
- `load_prompt_cache(cache_name)` - Load from current model cache
- `list_cached_prompts()` - List current model caches
- `clear_cache()` - Clear KV cache + system prompt

#### 3. **Add Cache API Endpoints** ğŸ”„
```python
POST /v1/cache/save
{
  "cache_name": "system_coding",
  "content": "You are a coding assistant..."
}

GET /v1/cache/list
Response: [{"cache_name": "system_coding", "created_at": 1729450123, ...}]

POST /v1/cache/load
{
  "cache_name": "system_coding"
}

DELETE /v1/cache/clear
```

#### 4. **Integrate with Chat Endpoint** ğŸ”„
```python
# In chat handler
POST /v1/chat/completions
{
  "model": "qwen3-0.6b",
  "messages": [...],
  "cache_prompt": "system_coding"  # NEW: Use cached system prompt
}
```

#### 5. **RKLLM Runtime Integration** âš ï¸
```python
# If RKLLM supports binary cache saving:
infer_param.prompt_cache_params = RKLLMPromptCacheParam()
infer_param.prompt_cache_params.save_prompt_cache = 1
infer_param.prompt_cache_params.prompt_cache_path = b"/cache/qwen3-0.6b/system_default.bin"
```

**Note**: Check if RKLLM 1.2.1 supports `rkllm_load_prompt_cache()` function!

---

## ğŸ¯ Expected Performance Improvements

### Current (No Caching)
- **TTFT**: ~214ms (Qwen3-0.6B with system prompt)
- **Workflow**: System prompt processed on every request

### With Caching
- **TTFT**: ~100-120ms (50% reduction) âœ¨
- **Workflow**: System prompt prefill cached, only user message processed

### Metrics to Track
```python
{
  "cache_hit": true,
  "ttft_ms": 105.3,          # Was 214ms
  "prefill_time_ms": 50.2,   # Was 150ms
  "cache_name": "system_coding"
}
```

---

## ğŸ“ Code Snippets to Adapt

### 1. Model Name Extraction (Copy to model_manager.py)
```python
def _create_friendly_name(self, filename: str) -> str:
    """Create friendly name - ADAPTED from technocore"""
    name = filename.replace('.rkllm', '').lower()
    
    if 'qwen3' in name and '0.6b' in name:
        return 'qwen3-0.6b'
    elif 'qwen3' in name and '4b' in name:
        return 'qwen3-4b'
    elif 'gemma' in name and '1b' in name:
        return 'gemma3-1b'
    # ... more patterns
    
    return name.split('-')[0]  # Fallback
```

### 2. Cache Directory Structure (Add to rkllm_model.py)
```python
class RKLLMModel:
    def __init__(self, ...):
        self.cache_base_dir = "cache"
        self.loaded_model_name = None
        os.makedirs(self.cache_base_dir, exist_ok=True)
    
    def _resolve_cache_path(self) -> str:
        """Get cache dir for current model"""
        if not self.loaded_model_name:
            raise RuntimeError("No model loaded")
        return os.path.join(self.cache_base_dir, self.loaded_model_name)
```

### 3. Cache Save/Load (Add to rkllm_model.py)
```python
def save_prompt_cache(self, cache_name: str, content: str) -> None:
    """Save prompt cache with metadata"""
    cache_dir = self._resolve_cache_path()
    os.makedirs(cache_dir, exist_ok=True)
    
    # Save text
    with open(f"{cache_dir}/{cache_name}.txt", 'w') as f:
        f.write(content)
    
    # Save metadata
    metadata = {
        "cache_name": cache_name,
        "model_name": self.loaded_model_name,
        "created_at": time.time(),
        "content_length": len(content)
    }
    with open(f"{cache_dir}/{cache_name}.json", 'w') as f:
        json.dump(metadata, f, indent=2)

def load_prompt_cache(self, cache_name: str) -> Optional[str]:
    """Load cached prompt"""
    cache_dir = self._resolve_cache_path()
    cache_path = f"{cache_dir}/{cache_name}.txt"
    
    if os.path.exists(cache_path):
        with open(cache_path, 'r') as f:
            return f.read()
    return None
```

---

## âš ï¸ Important Considerations

### 1. RKLLM Binary Cache Support
**Status**: UNCLEAR if RKLLM 1.2.1 fully supports `prompt_cache_params`  
**Action**: Test with RKLLM runtime to verify if binary caching works  
**Fallback**: Text-based cache works for chat template reuse

### 2. Memory Overhead
- **Cache Storage**: ~1-5 KB per cached prompt (negligible)
- **RKLLM KV Cache**: May require additional RAM per cache
- **Recommendation**: Limit to 5-10 cached prompts per model

### 3. Cache Invalidation
- **Parameter Changes**: Clear cache if inference params change
- **Model Update**: Clear cache on model version update
- **Manual Clear**: Provide API endpoint

### 4. Multi-User Scenarios
- **Current**: Single-user caching (good for Phase 4.1)
- **Future**: Add user-scoped caching if needed

---

## ğŸ¬ Next Steps for Implementation

1. âœ… **Copy Core Cache Methods** â†’ `src/models/rkllm_model.py`
2. âœ… **Create Cache Directory Structure** â†’ `cache/{model_name}/`
3. ğŸ”„ **Add Cache API Endpoints** â†’ `src/api/cache_routes.py`
4. ğŸ”„ **Integrate with Chat Handler** â†’ Support `cache_prompt` parameter
5. ğŸ”„ **Test RKLLM Binary Caching** â†’ Verify `prompt_cache_params` works
6. ğŸ”„ **Benchmark TTFT Improvements** â†’ Measure before/after
7. ğŸ”„ **Document Configuration** â†’ User guide for cache management

---

## ğŸ“š References

- **Source Code**: `/home/angeiv/AI/technocore/modules/rkllm/app/rkllm_engine.py`
- **RKLLM Structures**: Lines 23-157 (complete ctypes bindings)
- **Cache Implementation**: Lines 290-434 (save/load/list methods)
- **Context Detection**: Lines 256-288 (auto-detection logic)
- **Chat Template**: Lines 436-451 (system prompt API)

---

**Status**: Ready for implementation in Phase 4.1 ğŸš€
