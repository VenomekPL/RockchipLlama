# Benchmark Results

This folder contains comprehensive benchmark results for all models tested on the RK3588 NPU.

## Latest Benchmark Run: 2025-10-20

**Configuration:**
- Max Context Length: 16384 tokens
- NPU Cores: 3 (RK3588)
- Test Suite: 10 prompts (5 performance + 5 quality tests)

## Models Tested

### 1. google_gemma-3-270m (616 MB)
- **File**: `benchmark_20251020_w7wxn9.md`
- **TTFT**: 2.85 ms (average)
- **Range**: 2.12 - 7.17 ms
- **Success Rate**: 10/10 ‚úÖ

### 2. Qwen_Qwen3-0.6B (909 MB)
- **File**: `benchmark_20251020_28iu2q.md`
- **TTFT**: 2.37 ms (average) üèÜ **FASTEST**
- **Range**: 2.16 - 2.76 ms
- **Success Rate**: 10/10 ‚úÖ

### 3. gemma-3-1b-it (1.5 GB)
- **File**: `benchmark_20251020_kpm3vs.md`
- **TTFT**: 4.85 ms (average)
- **Range**: 3.95 - 7.34 ms
- **Success Rate**: 10/10 ‚úÖ

## Performance Comparison

| Model | Size | Avg TTFT | Min TTFT | Max TTFT | Consistency |
|-------|------|----------|----------|----------|-------------|
| Qwen 0.6B | 909 MB | **2.37 ms** ‚ö° | 2.16 ms | 2.76 ms | Excellent |
| Gemma 270M | 616 MB | 2.85 ms | 2.12 ms | 7.17 ms | Good |
| Gemma 1B | 1.5 GB | 4.85 ms | 3.95 ms | 7.34 ms | Good |

## Key Findings

### Speed Winner: Qwen 0.6B üèÜ
- Most consistent performance (smallest range: 0.6ms)
- Best average TTFT at 2.37 ms
- Mid-size model (909 MB)

### Smallest Model: Gemma 270M
- Surprisingly fast despite smallest size (616 MB)
- Some variance in initial requests (7.17 ms max)
- Good for resource-constrained scenarios

### Largest Model: Gemma 1B
- Slowest but most capable (when real inference is enabled)
- Larger context window potential
- Trade-off: Speed vs. capability

## Test Categories

### Performance Tests (5)
1. **Technical Explanation** - Short factual response
2. **Creative Writing** - Story beginning
3. **Scientific Discussion** - Complex explanation
4. **Historical Analysis** - Multi-paragraph analysis
5. **Technology Trends** - Opinion + reasoning

### Quality Tests (5)
6. **Creativity** - Fantasy story (400+ words)
7. **Planning** - Development roadmap
8. **Coding** - Python function implementation
9. **Instruction Following** - Strict JSON formatting
10. **Consistency** - Long-form essay (800+ words)

## File Format

Each benchmark run generates:
- **Markdown Report** (`.md`) - Human-readable with prompts, responses, and metrics
- **JSON Data** (`.json`) - Machine-readable detailed metrics

Filename format: `benchmark_YYYYMMDD_nonce.{md,json}`

## Notes

**Current Status**: Phase 2 - Mock responses
- API endpoints are functional
- Model loading/management works
- Timing measurements are accurate
- **Responses are placeholders** (empty strings)

**Next Phase**: Phase 3 - Real RKLLM Integration
- Will populate actual responses
- Quality comparison will be meaningful
- Token generation speed will be measured
- Response quality can be evaluated

## Usage

To run a new benchmark:

```bash
# Single model
python scripts/benchmark_full.py --model <model_name>

# All models
python scripts/benchmark_full.py --all-models

# Custom settings
python scripts/benchmark_full.py --all-models --max-context 8192 --npu-cores 2
```

## Quality Comparison (Coming in Phase 3)

Once real RKLLM integration is complete, these reports will include:
- Full prompt text for each test
- Complete model responses
- Side-by-side comparison of outputs
- Qualitative assessment of:
  - Creativity and coherence
  - Instruction following accuracy
  - Code quality and correctness
  - Consistency in long-form content
  - JSON formatting compliance

This will enable human evaluation of which model performs best for different use cases beyond just speed metrics.
